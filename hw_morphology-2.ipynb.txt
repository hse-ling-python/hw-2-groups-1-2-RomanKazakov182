{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2.1 по теме: морфология"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании обработываю текст **романа И. С. Тургенева \"Дворянское гнездо\"** с помощью морфологического анализатора, делаю поверхностный анализ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно же, проверяю код по PEP-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Текст книги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распаковывыю файл 'Dvorjanskoe_gnezdo.txt' (приложен к домашнему заданию) в формате .txt, содержащий текст книги. Присваиваю тексту переменную *text*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dvorjanskoe_gnezdo.txt', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортирую модуль *pymystem3*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ! pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункты 2.1 и 2.2. Парсинг и время"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаю класс *Mystem* и с помощью метода *analize* произвожу полный разбор текста. Разбору присваиваю переменную *ana*. С помощью magic command *%%time* замеряю время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 275 ms, sys: 238 ms, total: 513 ms\n",
      "Wall time: 3.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ana = m.analyze(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 2.3. Сохранение в json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортирую модуль *json*. Сохраняю массив *ana* в файл с помощью метода *dump*. **Ответ на задание – файл 'mystem_json.json', в котором хранится результат разбора (приложен к домашнему заданию).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('mystem_json.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(ana, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Pymorphy и nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pymorphy2\n",
    "# ! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункты 3.1, 3.2 и 3.3. Разбор текста на токены, pymorphy-разбор и время"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортирую модуль *nltk*. Токенизирую текст с помощью *word_tokenize*, привожу к нижнему регистру и сохраняю получившийся список как *words*. Импортирую модуль *pymorhy2*. С помощью класса *MorphAnalyzer()* и метода *parse* провожу разбор каждого слова-токена. Сохраняю разбор в список *analize_list*. С помощью magic command *%%time* замеряю время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.79 s, sys: 54.2 ms, total: 9.85 s\n",
      "Wall time: 9.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "\n",
    "analize_list = []\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()   \n",
    "for token in words:\n",
    "    token_analized = morph.parse(token)[0]\n",
    "    analize_list.append(token_analized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункты 3.4 и 3.5\\*. Сохранение в json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Для задания 4 (следующего):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью метода *tag.POS* получаю часть речи, а с помощью метода *normal_form* – лемму. Создаю из них кортеж и  добавляю такие кортежи в список *lemmas_POS*. Ещё создаём список частей речи *pos_list*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Для задания 3 (этого):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью  методов *word*, *normal_form*, *tag* получаю соответствующую информацию. Создаю кортежи типа (название параметра, его значение). Для грамматических характеристик (*tag*) создаю список с помощью *split(',')*. Сохраняю в указанном выше порядке кортежи в список *final_list*, из которого создаю *final_dict* – словарь, содержащий всю информацию о токене. Добавляю словарь в список *grand_list*. Записываю спикок-разбор в файл json. **Ответ на задание – файл 'pymorphy_json.json', в котором хранится результат разбора (приложен к домашнему заданию).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Для задания 5 (ещё более следующего; извините, что это всё так неудобно и не по порядку, просто таким образом программа будет работать быстрее, так как будет только один цикл):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью метода *normal_form* в список *lemmas* сохраняем все леммы в том порядке в котором они идут в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_POS = []\n",
    "pos_list = []\n",
    "grand_list = []\n",
    "lemmas = []\n",
    "for ana in analize_list:\n",
    "    # Для задания 4:\n",
    "    pos = str(ana.tag.POS)\n",
    "    pos_list.append(pos)\n",
    "    special_list = []\n",
    "    special_list.append(ana.normal_form)\n",
    "    special_list.append(pos)\n",
    "    special_tuple = tuple(special_list)\n",
    "    lemmas_POS.append(special_tuple)\n",
    "    # Для задания 3:\n",
    "    list_1 = []\n",
    "    list_1.append('word')\n",
    "    list_1.append(ana.word)\n",
    "    tuple_1 = tuple(list_1)\n",
    "    list_2 = []\n",
    "    list_2.append('normal_form')\n",
    "    list_2.append(ana.normal_form)\n",
    "    tuple_2 = tuple(list_2)\n",
    "    list_3 = []\n",
    "    list_3.append('tag')\n",
    "    tag = str(ana.tag)\n",
    "    tag = tag.split(',')\n",
    "    list_3.append(tag)\n",
    "    tuple_3 = tuple(list_3)\n",
    "    final_list = []\n",
    "    final_list.append(list_1)\n",
    "    final_list.append(list_2)\n",
    "    final_list.append(list_3)\n",
    "    final_dict = dict(final_list)\n",
    "    grand_list.append(final_dict)\n",
    "    # Для задания 5:\n",
    "    lemmas.append(ana.normal_form)\n",
    "with open('pymorphy_json.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(grand_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Отеты на вопросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 4.1. Какую долю слов составляет каждая часть речи?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем модуль *collections*. И по стандартной схеме с помощью *Counter* и *most_common* создаём частотный словарь, а потом и список из кортежей *common* (часть речи, количество употреблений). **Ответы на задание – список из кортежей *common* и составленный программой топ-20.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "№\tЧАСТЬ РЕЧИ\tЧАСТОТНОСТЬ\n",
      "\n",
      "1.\tNOUN\t\t0.239\n",
      "2.\tVERB\t\t0.149\n",
      "3.\tNPRO\t\t0.112\n",
      "4.\tADJF\t\t0.111\n",
      "5.\tPREP\t\t0.1\n",
      "6.\tCONJ\t\t0.097\n",
      "7.\tADVB\t\t0.062\n",
      "8.\tPRCL\t\t0.056\n",
      "9.\tINFN\t\t0.023\n",
      "10.\tADJS\t\t0.011\n",
      "11.\tGRND\t\t0.01\n",
      "12.\tNone\t\t0.008\n",
      "13.\tPRTF\t\t0.007\n",
      "14.\tNUMR\t\t0.005\n",
      "15.\tPRED\t\t0.003\n",
      "16.\tCOMP\t\t0.002\n",
      "17.\tINTJ\t\t0.002\n",
      "18.\tPRTS\t\t0.002\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "all_words = len(words)\n",
    "count_pos = collections.Counter(pos_list)\n",
    "common = count_pos.most_common()\n",
    "n = 0\n",
    "print('№\\tЧАСТЬ РЕЧИ\\tЧАСТОТНОСТЬ\\n')\n",
    "for p in common:\n",
    "    n += 1\n",
    "    freq = str(round(p[1]/all_words, 3))\n",
    "    print(str(n) + '.\\t' + p[0] + '\\t\\t' + freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 4.2. Топ-20 глаголов и наречий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из списка кортежей *lemmas_POS* отбираем леммы по частям речи *VERB* и *ADVB*. Затем по стандартной схеме с помощью модуля *collections* составляем частотные словари, списки с помощью *most_common* и топы. **Ответы на задание – топ-20 глаголов (ниже) и список common_verb, топ-20 наречий (ниже) и список common_adv.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТОП-20 ГЛАГОЛОВ:\n",
      "\n",
      "1. быть (389 вхождений)\n",
      "2. мочь (138 вхождений)\n",
      "3. говорить (99 вхождений)\n",
      "4. сказать (91 вхождений)\n",
      "5. стать (90 вхождений)\n",
      "6. хотеть (85 вхождений)\n",
      "7. знать (85 вхождений)\n",
      "8. промолвить (68 вхождений)\n",
      "9. возразить (55 вхождений)\n",
      "10. любить (53 вхождений)\n",
      "11. думать (49 вхождений)\n",
      "12. спросить (48 вхождений)\n",
      "13. проговорить (46 вхождений)\n",
      "14. сидеть (42 вхождений)\n",
      "15. глядеть (41 вхождений)\n",
      "16. подумать (41 вхождений)\n",
      "17. войти (40 вхождений)\n",
      "18. выйти (37 вхождений)\n",
      "19. продолжать (37 вхождений)\n",
      "20. начать (37 вхождений)\n",
      "\n",
      "\n",
      "ТОП-20 НАРЕЧИЙ:\n",
      "\n",
      "20. уже (98 вхождений)\n",
      "20. очень (98 вхождений)\n",
      "20. теперь (60 вхождений)\n",
      "20. вдруг (52 вхождений)\n",
      "20. опять (51 вхождений)\n",
      "20. несколько (48 вхождений)\n",
      "20. потом (45 вхождений)\n",
      "20. наконец (45 вхождений)\n",
      "20. тотчас (44 вхождений)\n",
      "20. где (43 вхождений)\n",
      "20. хорошо (42 вхождений)\n",
      "20. тоже (41 вхождений)\n",
      "20. почти (40 вхождений)\n",
      "20. скоро (38 вхождений)\n",
      "20. долго (34 вхождений)\n",
      "20. много (32 вхождений)\n",
      "20. давно (30 вхождений)\n",
      "20. тут (29 вхождений)\n",
      "20. немного (28 вхождений)\n",
      "20. никогда (28 вхождений)\n"
     ]
    }
   ],
   "source": [
    "verb_list = []\n",
    "adv_list = []\n",
    "for part in lemmas_POS:\n",
    "    if part[1] == 'VERB':\n",
    "        verb_list.append(part[0])\n",
    "    elif part[1] == 'ADVB':\n",
    "        adv_list.append(part[0])\n",
    "count_verb = collections.Counter(verb_list)\n",
    "common_verb = count_verb.most_common(20)\n",
    "n_1 = 0\n",
    "print('ТОП-20 ГЛАГОЛОВ:\\n')\n",
    "for vb in common_verb:\n",
    "    n_1 += 1\n",
    "    print(str(n_1) + '.', vb[0], '(' + str(vb[1]) + ' вхождений)')\n",
    "count_adv = collections.Counter(adv_list)\n",
    "print('\\n\\nТОП-20 НАРЕЧИЙ:\\n')\n",
    "common_adv = count_adv.most_common(20)\n",
    "n_2 = 0\n",
    "for adv in common_adv:\n",
    "    n_2 += 1\n",
    "    print(str(n_1) + '.', adv[0], '(' + str(adv[1]) + ' вхождений)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Топ-25 биграмм и триграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем метод *ngrams*. Создаём список *bigrams*. И с помощью указанного метода (указываем параметр 2, который знчит, что речь идёт именно о **би**граммах) и метода списков *extend*  все биграммы (лемматизированные – пользуемся списом *lemmas*). По старой схеме с помощью *collections* формируем частотный словарь и список кортежей *coomon_bi*. Составляем топ. Всё то же самое делаем для триграмм. **Ответы — для биграмм: список *common_bi* и топ-25 (ниже); для триграмм список *common_tri* и топ-25 (ниже).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТОП-25 БИГРАММ\n",
      "\n",
      "1. марья дмитрий (163 вхождений)\n",
      "2. варвар павло (124 вхождений)\n",
      "3. марфа тимофей (99 вхождений)\n",
      "4. он не (83 вхождений)\n",
      "5. не мочь (77 вхождений)\n",
      "6. и не (75 вхождений)\n",
      "7. я не (74 вхождений)\n",
      "8. на он (59 вхождений)\n",
      "9. она не (53 вхождений)\n",
      "10. и в (52 вхождений)\n",
      "11. не быть (50 вхождений)\n",
      "12. он в (49 вхождений)\n",
      "13. иван пётр (48 вхождений)\n",
      "14. он и (47 вхождений)\n",
      "15. она и (46 вхождений)\n",
      "16. что я (44 вхождений)\n",
      "17. фёдор иван (43 вхождений)\n",
      "18. ничто не (41 вхождений)\n",
      "19. вы не (41 вхождений)\n",
      "20. я вы (41 вхождений)\n",
      "21. она в (40 вхождений)\n",
      "22. у она (39 вхождений)\n",
      "23. он быть (38 вхождений)\n",
      "24. но он (38 вхождений)\n",
      "25. не знать (38 вхождений)\n",
      "\n",
      "\n",
      "\n",
      "ТОП-25 ТРИГРАММ\n",
      "\n",
      "1. в то же (14 вхождений)\n",
      "2. то же время (14 вхождений)\n",
      "3. в тот же (14 вхождений)\n",
      "4. марья дмитрий и (12 вхождений)\n",
      "5. но он не (12 вхождений)\n",
      "6. я не мочь (11 вхождений)\n",
      "7. и марфа тимофей (10 вхождений)\n",
      "8. он не мочь (10 вхождений)\n",
      "9. по крайний мера (10 вхождений)\n",
      "10. в один раз (10 вхождений)\n",
      "11. варвар павло и (10 вхождений)\n",
      "12. и в то (9 вхождений)\n",
      "13. не знать что (9 вхождений)\n",
      "14. на другой день (9 вхождений)\n",
      "15. я не знать (8 вхождений)\n",
      "16. в это мгновение (8 вхождений)\n",
      "17. в последний раз (8 вхождений)\n",
      "18. глядеть на он (8 вхождений)\n",
      "19. в сад и (7 вхождений)\n",
      "20. к марья дмитрий (7 вхождений)\n",
      "21. тот же день (7 вхождений)\n",
      "22. он и не (7 вхождений)\n",
      "23. то и дело (7 вхождений)\n",
      "24. марья дмитрий в (6 вхождений)\n",
      "25. она не быть (6 вхождений)\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "bigrams = []\n",
    "bigrams.extend(list(ngrams(lemmas, 2)))\n",
    "count_bi = collections.Counter(bigrams)\n",
    "common_bi = count_bi.most_common(25)\n",
    "print('ТОП-25 БИГРАММ\\n')\n",
    "n_bi = 0\n",
    "for bi in common_bi:\n",
    "    bi[0][0]\n",
    "    n_bi += 1\n",
    "    print(str(n_bi) + '.', bi[0][0], bi[0][1], '(' + str(bi[1]) +\n",
    "          ' вхождений)')\n",
    "print('\\n\\n')\n",
    "trigrams = []\n",
    "trigrams.extend(list(ngrams(lemmas, 3)))\n",
    "count_tri = collections.Counter(trigrams)\n",
    "common_tri = count_tri.most_common(25)\n",
    "print('ТОП-25 ТРИГРАММ\\n')\n",
    "n_tri = 0\n",
    "for tri in common_tri:\n",
    "    n_tri += 1\n",
    "    print(str(n_tri) + '.', tri[0][0], tri[0][1], tri[0][2],\n",
    "          '(' + str(tri[1]) + ' вхождений)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы**: среди биграмм встречаются, в основном, сочетания местоимений, предлогов, союзов, частицы *не* и (редко) глаголов. Интересный момент заключается в биграммах, представляющих собой сочетание имени и отчества персонажей; большинство среди триграмм составляют обстоятельства (часто временные), например *в это мгновение*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
